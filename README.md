# ML Assignment 2 – Multiple Classifiers + Streamlit App

## a) Problem Statement
The goal of this assignment is to build and compare multiple machine learning classification models using a single public dataset. Models must be evaluated with required metrics and demonstrated through an interactive **Streamlit** application.  
The app should support:
- Uploading test CSV data  
- Selecting a classifier  
- Viewing evaluation metrics  
- Displaying confusion matrices and classification reports  

## b) Dataset Description
**Dataset:** *Breast Cancer Wisconsin (Diagnostic)* – UCI Machine Learning Repository  

- Task: Binary classification (malignant vs. benign)  
- Size: **569 samples**, **30 numerical features**  
- Source: Features computed from digitized FNA images of breast masses, describing cell nuclei properties  

References:  
- [UCI dataset page](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)  
- [scikit-learn loader](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)  

**Target encoding:** `1 = malignant`, `0 = benign`

## c) Models Implemented & Metrics
Models trained on the dataset:  
1. Logistic Regression  
2. Decision Tree Classifier  
3. K-Nearest Neighbors (kNN)  
4. Gaussian Naive Bayes  
5. Random Forest (Ensemble)  
6. XGBoost (Ensemble)  

**Evaluation metrics:** Accuracy, AUC, Precision, Recall, F1 score, Matthews Correlation Coefficient (MCC)

### Comparison (Holdout Test Set)
Generated by `src/train.py` → stored in `model/model_comparison_metrics.csv`

| Model | Accuracy | AUC | Precision | Recall | F1 | MCC |
|---|---:|---:|---:|---:|---:|---:|
| Logistic Regression | 0.9649 | 0.9960 | 0.9750 | 0.9286 | 0.9512 | 0.9245 |
| Decision Tree | 0.9298 | 0.9246 | 0.9048 | 0.9048 | 0.9048 | 0.8492 |
| kNN | 0.9561 | 0.9825 | 0.9744 | 0.9048 | 0.9383 | 0.9058 |
| Naive Bayes | 0.9386 | 0.9934 | 1.0000 | 0.8333 | 0.9091 | 0.8715 |
| Random Forest | 0.9737 | 0.9944 | 1.0000 | 0.9286 | 0.9630 | 0.9442 |
| XGBoost | 0.9649 | 0.9924 | 1.0000 | 0.9048 | 0.9500 | 0.9258 |

## d) Observations on Model Performance
| Model | Observation |
|---|---|
| Logistic Regression | Strong overall performance with balanced precision and recall. |
| Decision Tree | Captures non-linear patterns but prone to overfitting; weaker than ensembles. |
| kNN | Works well when classes are well-separated; sensitive to scaling. |
| Naive Bayes | Quick baseline; independence assumption limits accuracy with correlated features. |
| Random Forest | Excellent balance of accuracy and robustness; top performer here. |
| XGBoost | Boosted trees iteratively correct errors, yielding strong results. |

## e) Project Structure
